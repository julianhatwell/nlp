{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from gensim.models import FastText\n",
    "from gensim.models.fasttext import load_facebook_vectors\n",
    "\n",
    "import fasttext\n",
    "import io\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def long_tensor(arr):\n",
    "    return(torch.tensor(arr, dtype=torch.long).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset with text prepoc and presentation code\n",
    "class Textsequencer(torch.utils.data.Dataset):\n",
    "    def __init__(self, words):\n",
    "        # TO DO pre-processing here\n",
    "        self.sequence_length = None\n",
    "        self.words = words\n",
    "        word_counts = Counter(words)\n",
    "        self.vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "        self.vocab_size = len(self.vocab)\n",
    "\n",
    "        self.index_to_word = {index: word for index, word in enumerate(self.vocab)}\n",
    "        self.word_to_index = {word: index for index, word in enumerate(self.vocab)}\n",
    "        self.words_indexes = [self.word_to_index[w] for w in self.words]\n",
    "    \n",
    "    # text processing here\n",
    "    def get_skipgrams(self, context_size):\n",
    "\n",
    "        skipgrams = []\n",
    "        for i in range(context_size, len(self.words) - context_size):\n",
    "            context = (\n",
    "                [self.words[i - j - 1] for j in range(context_size)],\n",
    "                [self.words[i + j + 1] for j in range(context_size)]\n",
    "            )\n",
    "            context[0].reverse()\n",
    "            context = context[0] + context[1]\n",
    "            target = self.words[i]\n",
    "            skipgrams.append((context, target))\n",
    "        return(skipgrams)\n",
    "\n",
    "    # get the context tensors for training\n",
    "    def index_vector(self, context):\n",
    "        if isinstance(context, str):\n",
    "            context = [context]\n",
    "        return(context, [self.word_to_index[w] for w in context])\n",
    "    \n",
    "    # to present to the data loader\n",
    "    def __len__(self):\n",
    "        return len(self.words_indexes) - self.sequence_length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if not self.sequence_length:\n",
    "            raise(AttributeError(\"Set the sequence_length property to generate sequences of length n\"))\n",
    "        return (\n",
    "            self.words[index:index+self.sequence_length], # word sequence x\n",
    "            self.words_indexes[index:index+self.sequence_length], # word index sequence x\n",
    "            self.words[index+1:index+self.sequence_length+1], # word sequence y\n",
    "            self.words_indexes[index+1:index+self.sequence_length+1], # word index sequence y\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here set up a CBOW model to learn embeddings with pytorch\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, context_size, embedding_dim, hidden_dim):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.context_size = context_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # nn.Embedding is a matrix of learnable weights, one row per word\n",
    "        self.embeddings = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
    "        # embedding vectors to be projected into a hidden layer, context size * 2 because skip gram is bef-y-aft\n",
    "        self.linear1 = nn.Linear(self.context_size * 2 * self.embedding_dim, hidden_dim)\n",
    "        # finally fully connect and softmax to output is the target y in bef-y-aft\n",
    "        self.linear2 = nn.Linear(hidden_dim, self.vocab_size) # fully connect hidden layer to (log) softmax\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        out = self.embeddings(inputs)\n",
    "        out = out.view(1, -1) # multiple words in the skipgram.\n",
    "        out = self.linear1(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        out = F.log_softmax(out, dim=1) # log probabilities\n",
    "        return(out)\n",
    "\n",
    "# here set up a CBOW model to learn embeddings with pytorch\n",
    "class CBOW_conv(nn.Module):\n",
    "    def __init__(self, vocab_size, context_size, embedding_dim, hidden_dim):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.context_size = context_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # nn.Embedding is a matrix of learnable weights, one row per word\n",
    "        self.embeddings = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
    "        # embedding vectors pooled, elementwise\n",
    "        \n",
    "        \n",
    "        self.linear1 = nn.Linear(self.context_size * 2 * self.embedding_dim, hidden_dim)\n",
    "        # finally fully connect and softmax to output is the target y in bef-y-aft\n",
    "        self.linear2 = nn.Linear(hidden_dim, self.vocab_size) # fully connect hidden layer to (log) softmax\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        out = self.embeddings(inputs)\n",
    "        print(out.shape)\n",
    "        out = out.view(1, -1) # multiple words in the skipgram.\n",
    "        print(out.shape)\n",
    "        out = self.linear1(out)\n",
    "        print(out.shape)\n",
    "        out = F.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        print(out.shape)\n",
    "        out = F.log_softmax(out, dim=1) # log probabilities\n",
    "        return(out)    \n",
    "    \n",
    "\n",
    "# traning this type of model\n",
    "def learn_embeddings(model, loss_function, optimiser, text_sequencer, context_size, epochs):\n",
    "    \n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch, (context_words, target_word) in enumerate(text_sequencer.get_skipgrams(context_size)):\n",
    "\n",
    "            # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
    "            # into integer indices and wrap them in tensors)\n",
    "            _, context_idx = text_sequencer.index_vector(context_words)\n",
    "            _, target_id = text_sequencer.index_vector(target_word)\n",
    "            \n",
    "            # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
    "            # new instance, you need to zero out the gradients from the old\n",
    "            # instance\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Step 3. Run the forward pass, getting log probabilities over next\n",
    "            # words\n",
    "            log_probs = model(long_tensor(context_idx))\n",
    "\n",
    "            # Step 4. Compute your loss function. (Again, Torch wants the target\n",
    "            # word wrapped in a tensor)\n",
    "            loss = loss_function(log_probs, long_tensor(target_id))\n",
    "\n",
    "            # Step 5. Do the backward pass and update the gradient\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "\n",
    "            # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
    "            total_loss += loss.item()\n",
    "            if batch % 10000 == 0:\n",
    "                print({ 'epoch': epoch, 'batch': batch, 'loss': loss.item() })\n",
    "        losses.append(total_loss)\n",
    "        \n",
    "    model.eval()\n",
    "    return(model, losses)\n",
    "\n",
    "# similarity of two words using the pytorch embeddings\n",
    "def test(model, word_1, word_2, word_to_ix):\n",
    "    # test word similarity\n",
    "    word_1_vec = model.embeddings.weight[word_to_ix[word_1]]\n",
    "    word_2_vec = model.embeddings.weight[word_to_ix[word_2]]\n",
    "    \n",
    "    dot_product = word_1_vec.dot(word_2_vec)\n",
    "    norm = torch.norm(word_1_vec) * torch.norm(word_2_vec)\n",
    "    word_similarity = (dot_product / norm)\n",
    "    print(f\"Similarity between {word_1} & {word_2} : {word_similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple LSTM model: inject with a pre-trained embedding : lstm -> fully connected\n",
    "class emb_LSTM(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size, input_size, # number of words and embedding dimension\n",
    "                 lstm_layers, lstm_size, # lstm layer parameters\n",
    "                 dropout): # a further hyper-param\n",
    "        super(emb_LSTM, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.input_size = input_size\n",
    "        self.lstm_size = lstm_size\n",
    "        self.lstm_layers = lstm_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.input_size,\n",
    "            hidden_size=self.lstm_size,\n",
    "            num_layers=self.lstm_layers,\n",
    "            dropout=self.dropout,\n",
    "        )\n",
    "        self.fully_connected = nn.Linear(self.lstm_size, vocab_size)\n",
    "    \n",
    "    def forward(self, input_vec, prev_state):\n",
    "        out, state = self.lstm(input_vec, prev_state)\n",
    "        out = self.fully_connected(output)\n",
    "        out = F.log_softmax(out, dim=1) # log probabilities predicting the target word\n",
    "        return(out, state)\n",
    "\n",
    "# a standalone function to generate the initial state tensors (zeros)\n",
    "def get_init_state(lstm_layers, sequence_length, lstm_size):\n",
    "    init_state = torch.zeros(lstm_layers, sequence_length, lstm_size)\n",
    "    return(init_state.to(device), init_state.to(device))\n",
    "\n",
    "def train_lstm(model, loss_function, optimiser, data_loader, init_state, epochs):\n",
    "    \n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        state_h, state_c = init_state\n",
    "\n",
    "        for batch, (words_x, words_idx_x, words_y, words_idx_y) in enumerate(data_loader):\n",
    "            optimiser.zero_grad()\n",
    "\n",
    "            y_pred, (state_h, state_c) = model(x.to(device), (state_h, state_c))\n",
    "            loss = loss_function(y_pred.transpose(1, 2), y.to(device))\n",
    "\n",
    "            state_h = state_h.detach()\n",
    "            state_c = state_c.detach()\n",
    "\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            if batch % 56 == 0:\n",
    "                print({ 'epoch': epoch, 'batch': batch, 'loss': loss.item() })\n",
    "        losses.append(total_loss)\n",
    "    model.eval()\n",
    "    return(model, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare jokes data (needs pre-processing, lower, clean of punct)\n",
    "train_df = pd.read_csv('data/reddit-cleanjokes.csv')\n",
    "jokes = train_df['Joke']\n",
    "jokes_nested_lists = [j.split(\" \") for j in train_df['Joke']]\n",
    "jokes_words_list = [c for c in chain(*jokes_nested_lists)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'batch': 0, 'loss': 9.17879867553711}\n",
      "{'epoch': 0, 'batch': 10000, 'loss': 6.666481971740723}\n",
      "{'epoch': 0, 'batch': 20000, 'loss': 3.663606643676758}\n",
      "{'epoch': 1, 'batch': 0, 'loss': 3.471705913543701}\n",
      "{'epoch': 1, 'batch': 10000, 'loss': 2.1564955711364746}\n",
      "{'epoch': 1, 'batch': 20000, 'loss': 0.14438307285308838}\n",
      "{'epoch': 2, 'batch': 0, 'loss': 3.3639581203460693}\n",
      "{'epoch': 2, 'batch': 10000, 'loss': 4.9272260665893555}\n",
      "{'epoch': 2, 'batch': 20000, 'loss': 10.653604507446289}\n",
      "{'epoch': 3, 'batch': 0, 'loss': 2.041869878768921}\n",
      "{'epoch': 3, 'batch': 10000, 'loss': 1.6359328031539917}\n",
      "{'epoch': 3, 'batch': 20000, 'loss': 1.7291313409805298}\n",
      "{'epoch': 4, 'batch': 0, 'loss': 1.5142778158187866}\n",
      "{'epoch': 4, 'batch': 10000, 'loss': 1.0933905839920044}\n",
      "{'epoch': 4, 'batch': 20000, 'loss': 3.2296481132507324}\n",
      "{'epoch': 5, 'batch': 0, 'loss': 0.14421312510967255}\n",
      "{'epoch': 5, 'batch': 10000, 'loss': 2.8275954723358154}\n",
      "{'epoch': 5, 'batch': 20000, 'loss': 3.5698933601379395}\n",
      "{'epoch': 6, 'batch': 0, 'loss': 7.571859836578369}\n",
      "{'epoch': 6, 'batch': 10000, 'loss': 0.9314592480659485}\n",
      "{'epoch': 6, 'batch': 20000, 'loss': 2.1952567100524902}\n",
      "{'epoch': 7, 'batch': 0, 'loss': 0.02934405952692032}\n",
      "{'epoch': 7, 'batch': 10000, 'loss': 1.1591593027114868}\n",
      "{'epoch': 7, 'batch': 20000, 'loss': 2.554863452911377}\n",
      "{'epoch': 8, 'batch': 0, 'loss': 0.11250410228967667}\n",
      "{'epoch': 8, 'batch': 10000, 'loss': 3.113492488861084}\n",
      "{'epoch': 8, 'batch': 20000, 'loss': 0.6815276741981506}\n",
      "{'epoch': 9, 'batch': 0, 'loss': 8.980311393737793}\n",
      "{'epoch': 9, 'batch': 10000, 'loss': 4.609175682067871}\n",
      "{'epoch': 9, 'batch': 20000, 'loss': 1.063982367515564}\n",
      "{'epoch': 10, 'batch': 0, 'loss': 0.599467933177948}\n",
      "{'epoch': 10, 'batch': 10000, 'loss': 4.380578994750977}\n",
      "{'epoch': 10, 'batch': 20000, 'loss': 0.2624415159225464}\n",
      "{'epoch': 11, 'batch': 0, 'loss': 3.7115237712860107}\n",
      "{'epoch': 11, 'batch': 10000, 'loss': 2.948936700820923}\n",
      "{'epoch': 11, 'batch': 20000, 'loss': 0.03871425241231918}\n",
      "{'epoch': 12, 'batch': 0, 'loss': 0.6794335246086121}\n",
      "{'epoch': 12, 'batch': 10000, 'loss': 2.5116426944732666}\n",
      "{'epoch': 12, 'batch': 20000, 'loss': 0.010166526772081852}\n",
      "{'epoch': 13, 'batch': 0, 'loss': 0.0002910667099058628}\n",
      "{'epoch': 13, 'batch': 10000, 'loss': 2.6366138458251953}\n",
      "{'epoch': 13, 'batch': 20000, 'loss': 0.19764281809329987}\n",
      "{'epoch': 14, 'batch': 0, 'loss': 0.0003895000845659524}\n",
      "{'epoch': 14, 'batch': 10000, 'loss': 2.0285537242889404}\n",
      "{'epoch': 14, 'batch': 20000, 'loss': 0.9166959524154663}\n",
      "{'epoch': 15, 'batch': 0, 'loss': 0.18582122027873993}\n",
      "{'epoch': 15, 'batch': 10000, 'loss': 1.6419949531555176}\n",
      "{'epoch': 15, 'batch': 20000, 'loss': 0.0425848513841629}\n",
      "{'epoch': 16, 'batch': 0, 'loss': 0.04373800382018089}\n",
      "{'epoch': 16, 'batch': 10000, 'loss': 4.282998085021973}\n",
      "{'epoch': 16, 'batch': 20000, 'loss': 0.005086339078843594}\n",
      "{'epoch': 17, 'batch': 0, 'loss': 0.0006180283380672336}\n",
      "{'epoch': 17, 'batch': 10000, 'loss': 1.9914385080337524}\n",
      "{'epoch': 17, 'batch': 20000, 'loss': 0.027521926909685135}\n",
      "{'epoch': 18, 'batch': 0, 'loss': 0.005933290813118219}\n",
      "{'epoch': 18, 'batch': 10000, 'loss': 0.32873523235321045}\n",
      "{'epoch': 18, 'batch': 20000, 'loss': 0.11269912868738174}\n",
      "{'epoch': 19, 'batch': 0, 'loss': 0.065519779920578}\n",
      "{'epoch': 19, 'batch': 10000, 'loss': 2.153423309326172}\n",
      "{'epoch': 19, 'batch': 20000, 'loss': 3.1512444019317627}\n",
      "[177301.4446446536, 162967.2633368765, 157285.21045913023, 153421.34755876998, 151581.28804125517, 150232.13447377627, 148980.82149542373, 147874.5640346158, 147609.12805288262, 146980.19080852013, 146438.9085293908, 147104.59263902318, 146426.23493285847, 147018.60952662275, 147579.52997629624, 148148.62274080055, 148305.07943410918, 148920.60173102375, 150106.23814259208, 149717.3101656787]\n"
     ]
    }
   ],
   "source": [
    "dataset = Textsequencer(jokes_words_list)\n",
    "context_size = 2\n",
    "\n",
    "embedding_dim = 64\n",
    "hidden_dim = 32\n",
    "embedding_learning_rate = 0.05\n",
    "epochs = 20\n",
    "\n",
    "embedding = CBOW(vocab_size=dataset.vocab_size, context_size=context_size,\n",
    "             embedding_dim=embedding_dim, hidden_dim=hidden_dim)\n",
    "\n",
    "optimiser = optim.SGD(embedding.parameters(), lr=embedding_learning_rate)\n",
    "loss_function = nn.NLLLoss() # this is the loss function for multi-class with a log_softmax final layer\n",
    "\n",
    "embedding, losses = learn_embeddings(embedding, loss_function, optimiser, dataset, context_size, epochs)\n",
    "print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Parameter' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-92090263dce8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0membedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'Parameter' object is not callable"
     ]
    }
   ],
   "source": [
    "embedding.embeddings.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_size = 64\n",
    "lstm_layers = 2\n",
    "\n",
    "lstm_model = emb_LSTM(vocab_size=dataset.vocab_size, # vocab_size: num of words (output size)\n",
    "                input_size=embedding_dim, # input size must match the embedding dim used\n",
    "                lstm_size=64, lstm_layers=3, dropout=0.2) # lstm hyper-params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-1e31f71921ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mlstm_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlstm_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_lstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimiser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlstm_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-5258d6111255>\u001b[0m in \u001b[0;36mtrain_lstm\u001b[0;34m(model, loss_function, optimiser, data_loader, init_state, epochs)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mstate_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m             \u001b[0moptimiser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "sequence_length = 4\n",
    "dataset.sequence_length = sequence_length\n",
    "\n",
    "batch_size = 10\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "init_state = get_init_state(lstm_layers, sequence_length, lstm_size)\n",
    "\n",
    "lstm_learning_rate = 0.005\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimiser = optim.Adam(lstm_model.parameters(), lr=lstm_learning_rate)\n",
    "\n",
    "lstm_epochs = 50\n",
    "\n",
    "losses, lstm_model = train_lstm(lstm_model, loss_function, optimiser, data_loader, init_state, epochs=lstm_epochs)\n",
    "print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('What', 'did'), ('did', 'the'), ('the', 'bartender'), ('bartender', 'say')] [tensor([2, 8]), tensor([8, 0]), tensor([  0, 248]), tensor([248,  20])] [('did', 'the'), ('the', 'bartender'), ('bartender', 'say'), ('say', 'to')] [tensor([8, 0]), tensor([  0, 248]), tensor([248,  20]), tensor([20,  4])]\n"
     ]
    }
   ],
   "source": [
    "dataset = Textsequencer(jokes_words_list)\n",
    "sequence_length = 4\n",
    "dataset.sequence_length = sequence_length\n",
    "data_loader = DataLoader(dataset, batch_size=2)\n",
    "(words_x, words_idx_x, words_y, words_idx_y) = next(iter(data_loader))\n",
    "print(words_x, words_idx_x, words_y, words_idx_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-9615ba56fff5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Knock knock. Whos there?'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "def predict(dataset, model, text, next_words=100):\n",
    "    model.eval()\n",
    "\n",
    "    words = text.split(' ')\n",
    "    state_h, state_c = model.init_state(len(words))\n",
    "\n",
    "    for i in range(0, next_words):\n",
    "        x = torch.tensor([[dataset.word_to_index[w] for w in words[i:]]])\n",
    "        y_pred, (state_h, state_c) = model(x.to(device), (state_h, state_c))\n",
    "\n",
    "        last_word_logits = y_pred[0][-1]\n",
    "        p = torch.nn.functional.softmax(last_word_logits, dim=0).detach().cpu().numpy()\n",
    "        word_index = np.random.choice(len(last_word_logits), p=p)\n",
    "        words.append(dataset.index_to_word[word_index])\n",
    "\n",
    "    return words\n",
    "\n",
    "\n",
    "print(predict(dataset, model, text='Knock knock. Whos there?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next((i for i in range(3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor([[2,3,4], [234,234,234]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
