{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers, activation=\"tanh\", dropout=0.0):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        # Defining some parameters\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        #Defining the layers\n",
    "        # RNN Layer\n",
    "        self.rnn = nn.RNN(input_size, hidden_dim, n_layers,\n",
    "                          batch_first=True,\n",
    "                          nonlinearity=activation,\n",
    "                          dropout=dropout)   \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # This method generates the first hidden state of zeros which we'll use in the forward pass\n",
    "        return(torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # Initializing hidden state for first input using method defined below\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "\n",
    "        # Passing in the input and hidden state into the model and obtaining outputs\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        \n",
    "        # Reshaping the outputs such that it can be fit into the fully connected layer\n",
    "        out = out.contiguous().view(-1, self.hidden_dim)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return(out, hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(sequence, dict_size, seq_len, batch_size):\n",
    "    # Creating a multi-dimensional array of zeros with the desired output shape\n",
    "    features = np.zeros((batch_size, seq_len, dict_size), dtype=np.float32)\n",
    "    \n",
    "    # Replacing the 0 at the relevant character index with a 1 to represent that character\n",
    "    for i in range(batch_size):\n",
    "        for u in range(seq_len):\n",
    "            features[i, u, sequence[i][u]] = 1\n",
    "    return features\n",
    "\n",
    "# whitespace padding all sequences to same length\n",
    "def ws_pad(text, new_len):\n",
    "    # note that string * int will generate the string repeated int times.\n",
    "    return(f\"{text}{' ' * (new_len - len(text))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sequence: hey how are you?        \n",
      "Target Sequence: ey how are you?         \n",
      "Input Sequence: good i am fine, thank yo\n",
      "Target Sequence: ood i am fine, thank you\n",
      "Input Sequence: have a nice day!        \n",
      "Target Sequence: ave a nice day!         \n",
      "Input Sequence: you too!                \n",
      "Target Sequence: ou too!                 \n",
      "Input Sequence: under the bridge        \n",
      "Target Sequence: nder the bridge         \n"
     ]
    }
   ],
   "source": [
    "text = [\"hey how are you?\",\"good i am fine, thank you\", \"have a nice day!\", \"you too!\", \"under the bridge\"]\n",
    "\n",
    "# padding with white space so all are same length\n",
    "maxlen = len(max(text, key=len))\n",
    "text = [ws_pad(t, maxlen) for t in text]\n",
    "\n",
    "# Join all the sentences together and extract the unique characters from the combined sentences\n",
    "chars = set(''.join(text))\n",
    "\n",
    "# Creating a dictionary that maps integers to the characters\n",
    "int2char = dict(enumerate(chars))\n",
    "\n",
    "# Creating another dictionary that maps characters to integers\n",
    "char2int = {char: ind for ind, char in int2char.items()}\n",
    "\n",
    "# Creating lists that will hold our input and target sequences\n",
    "input_seq = []\n",
    "target_seq = []\n",
    "\n",
    "for i in range(len(text)):\n",
    "    # Remove last character for input sequence\n",
    "    input_seq.append(text[i][:-1])\n",
    "    \n",
    "    # Remove first character for target sequence\n",
    "    target_seq.append(text[i][1:])\n",
    "    print(\"Input Sequence: {}\\nTarget Sequence: {}\".format(input_seq[i], target_seq[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode to integer\n",
    "for i in range(len(text)):\n",
    "    input_seq[i] = [char2int[character] for character in input_seq[i]]\n",
    "    target_seq[i] = [char2int[character] for character in target_seq[i]]\n",
    "    \n",
    "# one hot encode\n",
    "dict_size = len(char2int)\n",
    "seq_len = maxlen - 1\n",
    "batch_size = len(text)\n",
    "\n",
    "# Input shape --> (Batch Size, Sequence Length, One-Hot Encoding Size)\n",
    "input_seq = one_hot_encode(input_seq, dict_size, seq_len, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the inputs and targets to torch tensors\n",
    "input_sq = torch.from_numpy(input_seq)\n",
    "target_sq = torch.Tensor(target_seq)\n",
    "\n",
    "input_sq = input_sq.to(device)\n",
    "target_sq = target_sq.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model with hyperparameters\n",
    "model = RNN(\n",
    "    input_size=dict_size, output_size=dict_size, # don't mess with these\n",
    "    hidden_dim=15, n_layers=2, activation=\"tanh\", dropout=0.1 # experiment with these\n",
    "    ) \n",
    "# move to device\n",
    "model.to(device)\n",
    "\n",
    "# Define additional hyperparameters\n",
    "n_epochs = 500\n",
    "lr=0.02\n",
    "\n",
    "# Define Loss, Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25/500............. Loss: 1.7273\n",
      "Epoch: 50/500............. Loss: 0.7160\n",
      "Epoch: 75/500............. Loss: 0.2761\n",
      "Epoch: 100/500............. Loss: 0.1056\n",
      "Epoch: 125/500............. Loss: 0.0887\n",
      "Epoch: 150/500............. Loss: 0.0773\n",
      "Epoch: 175/500............. Loss: 0.0528\n",
      "Epoch: 200/500............. Loss: 0.0355\n",
      "Epoch: 225/500............. Loss: 0.0420\n",
      "Epoch: 250/500............. Loss: 0.0260\n",
      "Epoch: 275/500............. Loss: 0.0347\n",
      "Epoch: 300/500............. Loss: 0.0199\n",
      "Epoch: 325/500............. Loss: 0.0251\n",
      "Epoch: 350/500............. Loss: 0.0457\n",
      "Epoch: 375/500............. Loss: 0.0397\n",
      "Epoch: 400/500............. Loss: 0.0385\n",
      "Epoch: 425/500............. Loss: 0.0239\n",
      "Epoch: 450/500............. Loss: 0.0174\n",
      "Epoch: 475/500............. Loss: 0.0230\n",
      "Epoch: 500/500............. Loss: 0.0176\n"
     ]
    }
   ],
   "source": [
    "# Training Run\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    optimizer.zero_grad() # Clears existing gradients from previous epoch\n",
    "    output, hidden = model(input_sq)\n",
    "    output = output.to(device)\n",
    "    loss = criterion(output, target_sq.view(-1).long())\n",
    "    loss.backward() # Does backpropagation and calculates gradients\n",
    "    optimizer.step() # Updates the weights accordingly\n",
    "    \n",
    "    if epoch%25 == 0:\n",
    "        print('Epoch: {}/{}.............'.format(epoch, n_epochs), end=' ')\n",
    "        print(\"Loss: {:.4f}\".format(loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_one(model, character):\n",
    "    # One-hot encoding our input to fit into the model\n",
    "    character = np.array([[char2int[c] for c in character]])\n",
    "    character = one_hot_encode(character, dict_size, character.shape[1], 1)\n",
    "    character = torch.from_numpy(character)\n",
    "    character = character.to(device)\n",
    "    \n",
    "    out, hidden = model(character) # _ = hidden state\n",
    "    print(hidden)\n",
    "    \n",
    "    prob = nn.functional.softmax(out[-1], dim=0).data\n",
    "    # Taking the class with the highest probability score from the output\n",
    "    char_ind = torch.max(prob, dim=0)[1].item()\n",
    "    return int2char[char_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_seq(model, out_len, start='hey'):\n",
    "    model.eval() # eval mode\n",
    "    start = start.lower()\n",
    "    # First off, run through the starting characters\n",
    "    chars = [ch for ch in start]\n",
    "    size = out_len - len(chars)\n",
    "    # Now pass in the previous characters and get a new one\n",
    "    for ii in range(size):\n",
    "        char = predict_one(model, chars)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0284, -0.8003, -0.0492,  0.1104, -0.4383,  0.7508, -0.6349,\n",
      "          -0.0290,  0.0487,  0.1591,  0.7267,  0.7501,  0.3882, -0.2377,\n",
      "          -0.9318]],\n",
      "\n",
      "        [[ 0.9552, -0.9528, -1.0000,  0.9870,  0.9703, -0.9577, -0.9946,\n",
      "           0.7940,  0.9179, -0.9998,  0.9994,  0.6558,  0.3912, -0.9842,\n",
      "           0.9217]]], device='cuda:0', grad_fn=<CudnnRnnBackward>)\n",
      "tensor([[[-0.8557,  0.4711,  0.8419,  0.0656, -0.9898, -0.9801,  0.2434,\n",
      "           0.2707, -0.7429, -0.9123,  0.9579,  0.7909, -0.9557, -0.6194,\n",
      "          -0.9767]],\n",
      "\n",
      "        [[-0.9997,  1.0000, -1.0000,  0.9897,  1.0000, -0.9491,  0.9997,\n",
      "           0.9995, -0.9892, -0.9984,  0.9853,  0.9981,  0.9996,  0.1147,\n",
      "          -0.3026]]], device='cuda:0', grad_fn=<CudnnRnnBackward>)\n",
      "tensor([[[ 0.5126,  0.5460, -0.8963, -0.9479,  0.8314, -0.1483, -0.7762,\n",
      "           0.5620,  0.7413, -0.2096,  0.1159, -0.8713,  0.2776, -0.8319,\n",
      "          -0.0027]],\n",
      "\n",
      "        [[-1.0000,  0.9870,  0.9997, -0.9693,  1.0000, -0.9474, -1.0000,\n",
      "           0.9996,  0.9995, -0.9933, -0.8724, -0.9997, -0.9942, -0.2291,\n",
      "          -0.9739]]], device='cuda:0', grad_fn=<CudnnRnnBackward>)\n",
      "tensor([[[-0.7406,  0.3607,  0.5581, -0.6551, -0.2197, -0.7330,  0.5259,\n",
      "          -0.4349,  0.0458,  0.8486, -0.7632, -0.1959,  0.9036,  0.8733,\n",
      "           0.7570]],\n",
      "\n",
      "        [[ 0.9944,  0.9974, -0.9473, -0.9949, -0.1869,  0.9983, -0.9564,\n",
      "           0.9853,  0.3902,  0.9723,  0.9999,  0.9959, -0.9532, -1.0000,\n",
      "           0.9689]]], device='cuda:0', grad_fn=<CudnnRnnBackward>)\n",
      "tensor([[[-0.9249,  0.7787, -0.6899,  0.9840,  0.6438,  0.3544, -0.9917,\n",
      "           0.7406,  0.4794, -0.7016, -0.3415,  0.7087,  0.5499,  0.6011,\n",
      "          -0.7836]],\n",
      "\n",
      "        [[-0.9479, -0.9960, -0.9954,  0.9996, -0.9845, -0.4155,  0.0973,\n",
      "          -0.7672,  0.8906,  0.9978,  0.9967, -1.0000,  0.9545,  0.9707,\n",
      "          -0.9532]]], device='cuda:0', grad_fn=<CudnnRnnBackward>)\n",
      "tensor([[[ 0.9663,  0.9141, -0.8832,  0.1374,  0.9815,  0.8032, -0.8059,\n",
      "          -0.7513,  0.9645, -0.6036, -0.3068, -0.9829,  0.9888,  0.8771,\n",
      "           0.7048]],\n",
      "\n",
      "        [[ 0.9974, -1.0000,  1.0000, -0.9990, -0.9944,  0.9115, -0.9999,\n",
      "           0.9930, -0.9707,  0.9976, -1.0000, -0.9834, -0.9722,  1.0000,\n",
      "          -0.9999]]], device='cuda:0', grad_fn=<CudnnRnnBackward>)\n",
      "tensor([[[-0.5592, -0.7173,  0.4610,  0.5315, -0.7106,  0.5623,  0.8573,\n",
      "          -0.7359, -0.0284,  0.4465,  0.0905,  0.6531,  0.5706,  0.8492,\n",
      "           0.8893]],\n",
      "\n",
      "        [[ 0.8071, -0.9913, -0.9995, -0.9666, -0.9987,  0.8637,  0.9991,\n",
      "          -0.9756, -0.9983,  0.9996,  0.9993,  0.9999,  0.9981, -0.9986,\n",
      "          -0.9107]]], device='cuda:0', grad_fn=<CudnnRnnBackward>)\n",
      "tensor([[[-0.8243, -0.7925,  0.5546,  0.4564,  0.9319, -0.9451, -0.9785,\n",
      "           0.7724, -0.7550,  0.7594, -0.8532,  0.9736, -0.9693, -0.3350,\n",
      "          -0.8703]],\n",
      "\n",
      "        [[-1.0000,  0.9977, -1.0000,  1.0000,  0.9728, -0.9984,  1.0000,\n",
      "           1.0000, -0.9968, -0.9997,  1.0000, -0.9364,  0.8010,  0.8978,\n",
      "          -1.0000]]], device='cuda:0', grad_fn=<CudnnRnnBackward>)\n",
      "tensor([[[ 0.8565,  0.4989, -0.1461,  0.5343, -0.5574, -0.3187,  0.1308,\n",
      "           0.9706, -0.8169,  0.8688, -0.5238, -0.0580, -0.0099, -0.9284,\n",
      "          -0.9885]],\n",
      "\n",
      "        [[-0.9999,  1.0000,  0.9999, -0.9860,  1.0000, -0.9983, -1.0000,\n",
      "           1.0000,  0.9995, -0.9994,  0.9642,  0.9138,  0.9538, -0.9198,\n",
      "          -0.9925]]], device='cuda:0', grad_fn=<CudnnRnnBackward>)\n",
      "tensor([[[ 0.9931,  0.7331, -0.6173,  0.1220, -0.5887,  0.0149,  0.9488,\n",
      "          -0.5788, -0.4412,  0.4323, -0.4424, -0.2799, -0.4021, -0.8627,\n",
      "          -0.8875]],\n",
      "\n",
      "        [[ 0.9969, -0.4146,  0.0683, -0.9500,  1.0000, -0.9561, -0.9999,\n",
      "           0.9719,  0.9911, -0.9995,  0.9995,  0.9968, -0.9971, -0.9808,\n",
      "           0.9646]]], device='cuda:0', grad_fn=<CudnnRnnBackward>)\n",
      "tensor([[[ 0.9856,  0.6571, -0.2367,  0.4973,  0.8086,  0.8121,  0.8125,\n",
      "           0.8334,  0.0066,  0.8121, -0.8437, -0.7981,  0.9857, -0.7039,\n",
      "          -0.0615]],\n",
      "\n",
      "        [[ 0.9882, -0.9929, -0.9732, -0.9998,  0.9449,  0.9994, -0.9992,\n",
      "          -0.2658,  0.9988,  0.9986,  0.9616, -0.9977, -0.9999,  0.9952,\n",
      "           0.5399]]], device='cuda:0', grad_fn=<CudnnRnnBackward>)\n",
      "tensor([[[ 0.0966, -0.6534, -0.7902,  0.4655,  0.2665,  0.6070, -0.8864,\n",
      "           0.6030,  0.8256,  0.9712,  0.4783,  0.8905,  0.5611, -0.7949,\n",
      "          -0.8500]],\n",
      "\n",
      "        [[-0.9924, -0.9991, -0.9928,  0.9999, -0.9980, -0.9998,  0.9939,\n",
      "          -0.9999,  0.9291,  0.9999, -0.9826, -0.3752,  0.8742,  0.9989,\n",
      "          -0.9981]]], device='cuda:0', grad_fn=<CudnnRnnBackward>)\n",
      "tensor([[[ 0.1893,  0.0738, -0.2090, -0.1606,  0.2332,  0.7354,  0.4756,\n",
      "          -0.5664, -0.1499,  0.0475,  0.2172, -0.8411,  0.4862,  0.0599,\n",
      "          -0.7950]],\n",
      "\n",
      "        [[ 0.9425, -0.4490,  0.9999, -0.9976, -0.9922, -0.9895, -0.1991,\n",
      "           0.9984, -0.9617, -0.9930, -0.9746,  0.9960,  0.9997,  1.0000,\n",
      "          -0.8847]]], device='cuda:0', grad_fn=<CudnnRnnBackward>)\n",
      "tensor([[[-0.9682, -0.6729,  0.9570,  0.2503, -0.9058, -0.9101,  0.9706,\n",
      "          -0.1101,  0.3627, -0.8320, -0.8103,  0.8639, -0.9037, -0.7203,\n",
      "          -0.5635]],\n",
      "\n",
      "        [[-0.9968,  0.9998, -1.0000, -0.8856,  0.9999, -0.9997,  0.9998,\n",
      "           0.9769, -0.9703, -1.0000,  1.0000,  0.9994,  0.9965, -1.0000,\n",
      "          -0.9504]]], device='cuda:0', grad_fn=<CudnnRnnBackward>)\n",
      "tensor([[[-0.7730,  0.4267, -0.9186, -0.8987,  0.8376, -0.6549, -0.9490,\n",
      "           0.8769,  0.6051,  0.9589, -0.3426, -0.0473, -0.4081, -0.9135,\n",
      "          -0.3691]],\n",
      "\n",
      "        [[-1.0000,  1.0000,  0.9420,  0.9978,  1.0000, -0.9997, -0.9959,\n",
      "           0.9991,  0.9998, -0.9995,  0.9975, -0.9995, -0.9793, -0.9991,\n",
      "          -0.9994]]], device='cuda:0', grad_fn=<CudnnRnnBackward>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'hey am fine, thank y'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_seq(model, 20, 'hey a').strip() # remove final white space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
